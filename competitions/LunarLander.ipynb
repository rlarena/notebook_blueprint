{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e20074-8c36-4e6b-a815-305ac6127638",
   "metadata": {},
   "source": [
    "# LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc9a32-2564-4c1d-8bf2-eec91eeb620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5794a-e345-4967-b503-510974d57d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!git clone https://github.com/rlarena/notebook_blueprint.git > /dev/null 2>&1\n",
    "!pip install -r notebook_blueprint/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d20c6-0e77-4406-8262-87719fd2498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with env\n",
    "import gymnasium as gym\n",
    "env = gym.make(env_name)\n",
    "observation, info = env.reset()\n",
    "cum_reward = 0\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    cum_reward += reward\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "print(f\"cum_reward:{cum_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af15288-8c27-4584-a437-fa2fd367b9a2",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69b12a-22be-4206-a0af-2532a78ea447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"my_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd656e-d351-4434-b184-82bd0bfdf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "# Save the agent\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f54434-895d-40a2-92a9-a6cfa946b0d0",
   "metadata": {},
   "source": [
    "## Publish agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84b03a-6404-4193-bfc7-ed55ec244919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        from stable_baselines3 import DQN\n",
    "        self.model =  DQN.load(model_path)\n",
    "\n",
    "    def choose_action(self, observation, action_mask=None):\n",
    "        action, _states = self.model.predict(observation, deterministic=True)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379efe26-0ba3-4b1a-ac3a-5654a3765757",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stringify_agent = f\"\"\"\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        from stable_baselines3 import DQN\n",
    "        self.model =  DQN.load(\"{model_path}\")\n",
    "\n",
    "    def choose_action(self, observation, action_mask=None):\n",
    "        action, _states = self.model.predict(observation, deterministic=True)\n",
    "        return action\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412ffd5-887a-4fa7-af0a-b47b4d39c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to https://rlarena.com/my-profile to get or generate your key pair\n",
    "key_id=\"\"\n",
    "key_pass=\"\"\n",
    "#\n",
    "agent_attach_name= \"my_super_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c44b3-9ff7-46cf-b033-e266cb2b59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "def attach_agent(key_id, key_pass, agent_attach_name, model_path):\n",
    "    # Endpoint URL\n",
    "    url = 'https://rlarena.com/api/direct_attache_agents_notebook/competition/1'\n",
    "\n",
    "    # Your credentials and agent details\n",
    "    data = {\n",
    "        'key_id': key_id,\n",
    "        'key_pass': key_pass,\n",
    "        'agent_attach_name': agent_attach_name,\n",
    "    }\n",
    "\n",
    "    # Files to upload: agent.py and the model\n",
    "    files = {\n",
    "        'model': (model_path, open(model_path, 'rb')),\n",
    "        'agent_code': ('agent.py', io.StringIO(my_stringify_agent)),\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    try:\n",
    "        response = requests.post(url, data=data, files=files)\n",
    "        # Ensure files are closed properly\n",
    "        files['model'][1].close()\n",
    "        files['agent_code'][1].close()\n",
    "        return response.json()  # Return the JSON response\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to attach agent due to: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "response = attach_agent(key_id, key_pass, agent_attach_name, model_path)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d885e-78c2-4faa-928c-4cddca56a7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
