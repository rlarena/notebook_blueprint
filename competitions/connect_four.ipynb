{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e20074-8c36-4e6b-a815-305ac6127638",
   "metadata": {},
   "source": [
    "# connect_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5794a-e345-4967-b503-510974d57d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install swig==4.2.1 2>&1\n",
    "!pip install gymnasium==0.29.1 2>&1\n",
    "!pip install pettingzoo==1.24.3 2>&1\n",
    "!pip install box2d-py==2.3.5 2>&1\n",
    "!pip install stable_baselines3==2.3.0 2>&1\n",
    "!pip install sb3_contrib==2.3.0 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdc9a32-2564-4c1d-8bf2-eec91eeb620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665d20c6-0e77-4406-8262-87719fd2498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'player_0': 1, 'player_1': -1}\n"
     ]
    }
   ],
   "source": [
    "env = connect_four_v3.env()\n",
    "env.reset(seed=42)\n",
    "cum_reward = {agent:0 for agent in env.agents}\n",
    "\n",
    "# interact with env\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    cum_reward[agent] += reward\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        mask = observation[\"action_mask\"]\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample(mask)\n",
    "    env.step(action)\n",
    "env.close()\n",
    "print(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbe9dc2-f164-4462-93a0-d83be9e1011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def choose_action(self, observation, action_mask=None):\n",
    "        if self.env.action_space.__class__.__name__ == \"method\":  # Multiplayer env\n",
    "            action = self.env.action_space(self.env.agents[0]).sample(action_mask)\n",
    "        else:  # Classic gymnasium\n",
    "            action = self.env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f586b2-2a37-483e-9975-2082d563aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False (6, 7, 2)\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 5 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      ". . . . . O .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 5 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      ". . . . . X .\n",
      ". . . . . O .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 0 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O . . . . O .\n",
      ". . . . . X .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 1 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O . . . X .\n",
      ". . . . . O .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 4 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X . . O O .\n",
      ". . . . . X .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 1 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O . . X X .\n",
      ". O . . . O .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 4 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X . . O O .\n",
      ". X . . O X .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 3 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O . O X X .\n",
      ". O . . X O .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 1 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X . X O O .\n",
      ". X . . O X .\n",
      ". O . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 3 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O . O X X .\n",
      ". O . O X O .\n",
      ". X . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 4 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X . X O O .\n",
      ". X . X O X .\n",
      ". O . . O . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 2 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O O O X X .\n",
      ". O . O X O .\n",
      ". X . . X . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 1 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X X X O O .\n",
      ". X . X O X .\n",
      ". O . . O . .\n",
      ". O . . . . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 4 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O O O X X .\n",
      ". O . O X O .\n",
      ". X . . X . .\n",
      ". X . . O . .\n",
      ". . . . . . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 4 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X X X O O .\n",
      ". X . X O X .\n",
      ". O . . O . .\n",
      ". O . . X . .\n",
      ". . . . O . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 6 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O O O X X O\n",
      ". O . O X O .\n",
      ". X . . X . .\n",
      ". X . . O . .\n",
      ". . . . X . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 1 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X X X O O X\n",
      ". X . X O X .\n",
      ". O . . O . .\n",
      ". O . . X . .\n",
      ". O . . O . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 0 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O O O X X O\n",
      "O O . O X O .\n",
      ". X . . X . .\n",
      ". X . . O . .\n",
      ". X . . X . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 3 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X X X O O X\n",
      "X X . X O X .\n",
      ". O . O O . .\n",
      ". O . . X . .\n",
      ". O . . O . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 5 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "X O O O X X O\n",
      "O O . O X O .\n",
      ". X . X X O .\n",
      ". X . . O . .\n",
      ". X . . X . .\n",
      ". . . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_0 1 0 False\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "{'player_0': 0, 'player_1': 0}\n",
      "O X X X O O X\n",
      "X X . X O X .\n",
      ". O . O O X .\n",
      ". O . . X . .\n",
      ". O . . O . .\n",
      ". O . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0', 'player_1']\n",
      "player_1 None -1 True\n",
      "['player_0', 'player_1'] ['player_0']\n",
      "{'player_0': 0, 'player_1': -1}\n",
      "X O O O X X O\n",
      "O O . O X O .\n",
      ". X . X X O .\n",
      ". X . . O . .\n",
      ". X . . X . .\n",
      ". X . . . . .\n",
      "\n",
      "['player_0', 'player_1'] ['player_0']\n",
      "player_0 None 1 True\n",
      "{'player_0': 1, 'player_1': -1}\n",
      "['player_0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = connect_four_v3.env()\n",
    "env.reset()\n",
    "cum_reward = {agent:0 for agent in env.agents}\n",
    "last_done = [False] * len(env.agents)\n",
    "observation, reward, termination, truncation, info = env.last()\n",
    "print(reward, termination, observation['observation'].shape)\n",
    "formatted_board = \"\"\n",
    "for row in reversed(range(observation['observation'].shape[0])):\n",
    "    row_string = \"\"\n",
    "    for col in range(observation['observation'].shape[1]):\n",
    "        if observation['observation'][row, col, 0] == 1:\n",
    "            row_string += \"X \"\n",
    "        elif observation['observation'][row, col, 1] == 1:\n",
    "            row_string += \"O \"\n",
    "        else:\n",
    "            row_string += \". \"\n",
    "    formatted_board += row_string.strip() + \"\\n\"\n",
    "        \n",
    "print(formatted_board)\n",
    "agents = list(env.agents)\n",
    "\n",
    "ft_agent = Agent(env)\n",
    "\n",
    "idx = 0\n",
    "# interact with env\n",
    "while not all(last_done):\n",
    "    for idx, agent in enumerate(agents):\n",
    "        cum_reward[agent] += reward\n",
    "        last_done[idx] = termination or truncation\n",
    "        if last_done[idx]:\n",
    "            action = None\n",
    "        else:\n",
    "            mask = observation[\"action_mask\"]\n",
    "            # this is where you would insert your policy\n",
    "            action = ft_agent.choose_action(observation['observation'], observation['action_mask'])#env.action_space(agent).sample(mask)\n",
    "        print(agents,env.agents)\n",
    "        print(agent, action, cum_reward[agent], last_done[idx])\n",
    "        if all (last_done):\n",
    "            break\n",
    "        env.step(action)\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        print(agents,env.agents)\n",
    "        \n",
    "        print(cum_reward)\n",
    "        \n",
    "        \n",
    "        formatted_board = \"\"\n",
    "\n",
    "        # Reverse to print from bottom to top (as Connect Four works from bottom row up)\n",
    "        for row in reversed(range(observation['observation'].shape[0])):\n",
    "            row_string = \"\"\n",
    "            for col in range(observation['observation'].shape[1]):\n",
    "                if observation['observation'][row, col, 0] == 1:\n",
    "                    row_string += \"X \"\n",
    "                elif observation['observation'][row, col, 1] == 1:\n",
    "                    row_string += \"O \"\n",
    "                else:\n",
    "                    row_string += \". \"\n",
    "            formatted_board += row_string.strip() + \"\\n\"\n",
    "        \n",
    "        print(formatted_board)\n",
    "        \n",
    "print(cum_reward)\n",
    "print(env.agents)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af15288-8c27-4584-a437-fa2fd367b9a2",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69b12a-22be-4206-a0af-2532a78ea447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"my_model.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd656e-d351-4434-b184-82bd0bfdf182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "import pettingzoo.utils\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\"\"\"\n",
    "        super().step(action)\n",
    "        return super().last()\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]\n",
    "\n",
    "\n",
    "def mask_fn(env):\n",
    "    return env.action_mask()\n",
    "\n",
    "\n",
    "def train_action_mask(env_fn, steps=10_000, seed=0, **env_kwargs):\n",
    "    \"\"\"Train a single model to play as each agent in a zero-sum game environment using invalid action masking.\"\"\"\n",
    "    env = env_fn.env(**env_kwargs)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    # Custom wrapper to convert PettingZoo envs to work with SB3 action masking\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "\n",
    "    env.reset(seed=seed)  # Must call reset() in order to re-define the spaces\n",
    "\n",
    "    env = ActionMasker(env, mask_fn)  # Wrap to enable masking (SB3 function)\n",
    "    # MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\n",
    "    # with ActionMasker. If the wrapper is detected, the masks are automatically\n",
    "    # retrieved and used when learning. Note that MaskablePPO does not accept\n",
    "    # a new action_mask_fn kwarg, as it did in an earlier draft.\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent. Trained agent will play as {env.possible_agents[1]}.\"\n",
    "    )\n",
    "\n",
    "    model = MaskablePPO.load(model_path)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    round_rewards = []\n",
    "\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                # If there is a winner, keep track, otherwise don't change the scores (tie)\n",
    "                if (\n",
    "                    env.rewards[env.possible_agents[0]]\n",
    "                    != env.rewards[env.possible_agents[1]]\n",
    "                ):\n",
    "                    winner = max(env.rewards, key=env.rewards.get)\n",
    "                    scores[winner] += env.rewards[\n",
    "                        winner\n",
    "                    ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                round_rewards.append(env.rewards)\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    # Avoid dividing by zero\n",
    "    if sum(scores.values()) == 0:\n",
    "        winrate = 0\n",
    "    else:\n",
    "        winrate = scores[env.possible_agents[1]] / sum(scores.values())\n",
    "    print(\"Rewards by round: \", round_rewards)\n",
    "    print(\"Total rewards (incl. negative rewards): \", total_rewards)\n",
    "    print(\"Winrate: \", winrate)\n",
    "    print(\"Final scores: \", scores)\n",
    "    return round_rewards, total_rewards, winrate, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6548b0-ad9e-41de-a0e6-789e0e97be4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_kwargs = {}\n",
    "env_fn = connect_four_v3\n",
    "train_action_mask(env_fn, steps=20_480, seed=0, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a4754-4632-4942-bbd0-a66ec0e428c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 100 games against a random agent\n",
    "res = eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f54434-895d-40a2-92a9-a6cfa946b0d0",
   "metadata": {},
   "source": [
    "## Publish agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84b03a-6404-4193-bfc7-ed55ec244919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        from sb3_contrib import MaskablePPO\n",
    "        self.model =  MaskablePPO.load(model_path)\n",
    "\n",
    "    def choose_action(self, observation, action_mask=None):\n",
    "        action, _states = self.model.predict(observation, deterministic=True)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379efe26-0ba3-4b1a-ac3a-5654a3765757",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stringify_agent = f\"\"\"\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        from sb3_contrib import MaskablePPO\n",
    "        self.model =  MaskablePPO.load(\"{model_path}\")\n",
    "\n",
    "    def choose_action(self, observation, action_mask=None):\n",
    "        action, _states = self.model.predict(observation, deterministic=True)\n",
    "        return action\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412ffd5-887a-4fa7-af0a-b47b4d39c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to https://rlarena.com/my-profile to get or generate your key pair\n",
    "key_id=\"\"\n",
    "key_pass=\"\"\n",
    "#\n",
    "agent_attach_name= \"my_super_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c44b3-9ff7-46cf-b033-e266cb2b59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "def attach_agent(key_id, key_pass, agent_attach_name, model_path):\n",
    "    # Endpoint URL\n",
    "    url = 'https://rlarena.com/api/direct_attache_agents_notebook/competition/3'\n",
    "\n",
    "    # Your credentials and agent details\n",
    "    data = {\n",
    "        'key_id': key_id,\n",
    "        'key_pass': key_pass,\n",
    "        'agent_attach_name': agent_attach_name,\n",
    "    }\n",
    "\n",
    "    # Files to upload: agent.py and the model\n",
    "    files = {\n",
    "        'model': (model_path, open(model_path, 'rb')),\n",
    "        'agent_code': ('agent.py', io.StringIO(my_stringify_agent)),\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    try:\n",
    "        response = requests.post(url, data=data, files=files)\n",
    "        # Ensure files are closed properly\n",
    "        files['model'][1].close()\n",
    "        files['agent_code'][1].close()\n",
    "        return response.json()  # Return the JSON response\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to attach agent due to: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "response = attach_agent(key_id, key_pass, agent_attach_name, model_path)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
