{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774cf814-1847-4363-be81-e439d9cc45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05904b7c-a7c3-4460-8138-66b45e927412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_tree_search(root, policy_net, max_seconds=3, max_milliseconds=0):\n",
    "    end_time = datetime.now() + timedelta(seconds=max_seconds, milliseconds=max_milliseconds)\n",
    "    node = root\n",
    "    while datetime.now() < end_time:\n",
    "        while node and not node.untried_moves and node.children:\n",
    "            current_available_moves = node.game_state.available_moves()\n",
    "            node = node.best_child(current_available_moves)\n",
    "        if node and node.untried_moves:\n",
    "            node = node.expand()\n",
    "        while node and not node.game_state.is_terminal():\n",
    "            possible_moves = node.game_state.available_moves()\n",
    "            chosen_move = random.choice(possible_moves)\n",
    "            node.game_state.make_move(chosen_move)\n",
    "            node.game_state.switch_player()\n",
    "        result = 1 if node and node.game_state.is_winner(node.player_just_moved) else 0\n",
    "        while node:\n",
    "            node.update(result)\n",
    "            node = node.parent\n",
    "    if root.children:\n",
    "        root_available_moves = root.game_state.available_moves()\n",
    "        best_child = root.best_child(root_available_moves)\n",
    "        return best_child.move if best_child else None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9296e6a7-eddd-4fdf-8101-1d810c0a4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(action_probabilities, action, outcome):\n",
    "    action_tensor = torch.tensor([action], dtype=torch.long)\n",
    "    outcome_tensor = torch.tensor([outcome], dtype=torch.float32)\n",
    "    loss = nn.CrossEntropyLoss()(action_probabilities, action_tensor) * outcome_tensor\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920ca7f0-6f9c-42b1-8991-aeb3d6734ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No trained model found. Starting training from scratch.\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n",
      "Input Tensor Shape: torch.Size([1, 2, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.board = [[0]*7 for _ in range(6)]\n",
    "        self.current_player = 1  # Start with player 1\n",
    "\n",
    "    def make_move(self, column):\n",
    "        if self.board[0][column] != 0:\n",
    "            return False  # Column is full\n",
    "        for row in reversed(range(6)):\n",
    "            if self.board[row][column] == 0:\n",
    "                self.board[row][column] = self.current_player\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def available_moves(self):\n",
    "        return [c for c in range(7) if self.board[0][c] == 0]\n",
    "\n",
    "    def is_winner(self, player):\n",
    "        for row in range(6):\n",
    "            for col in range(4):\n",
    "                if all(self.board[row][col+i] == player for i in range(4)):\n",
    "                    return True\n",
    "        for col in range(7):\n",
    "            for row in range(3):\n",
    "                if all(self.board[row+i][col] == player for i in range(4)):\n",
    "                    return True\n",
    "        for row in range(3):\n",
    "            for col in range(4):\n",
    "                if all(self.board[row+i][col+i] == player for i in range(4)):\n",
    "                    return True\n",
    "                if all(self.board[row+3-i][col+i] == player for i in range(4)):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def is_draw(self):\n",
    "        return all(self.board[0][col] != 0 for col in range(7))\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.is_winner(1) or self.is_winner(2) or self.is_draw()\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = 2 if self.current_player == 1 else 1\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 6 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 7)  # Output probabilities for each column\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(-1, 128 * 6 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, game_state, policy_net, move=None, parent=None):\n",
    "        self.game_state = deepcopy(game_state)\n",
    "        self.policy_net = policy_net\n",
    "        self.move = move\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.wins = 0\n",
    "        self.visits = 0\n",
    "        self.action_probabilities = None\n",
    "        self.player_just_moved = game_state.current_player\n",
    "        self.untried_moves = game_state.available_moves()\n",
    "\n",
    "    def state_to_tensor(self):\n",
    "        # Initialize a tensor for two channels. Each channel corresponds to one player.\n",
    "        state_tensor = torch.zeros((1, 2, 6, 7), dtype=torch.float32)  # Shape [batch_size, num_channels, height, width]\n",
    "        for r in range(6):\n",
    "            for c in range(7):\n",
    "                player = self.game_state.board[r][c]\n",
    "                if player == 1:\n",
    "                    state_tensor[0, 0, r, c] = 1  # Set player 1's layer\n",
    "                elif player == 2:\n",
    "                    state_tensor[0, 1, r, c] = 1  # Set player 2's layer\n",
    "        return state_tensor\n",
    "\n",
    "\n",
    "    # Ensure this method is called before feeding data into your neural network\n",
    "    def fetch_policy_probabilities(self):\n",
    "        input_tensor = self.state_to_tensor()\n",
    "        print(f\"Input Tensor Shape: {input_tensor.shape}\")  # Debugging line to verify tensor shape\n",
    "        with torch.no_grad():\n",
    "            output = self.policy_net(input_tensor)\n",
    "            self.action_probabilities = output.squeeze().numpy()\n",
    "\n",
    "    def select_probabilistic_move(self):\n",
    "        move_probabilities = {move: self.action_probabilities[move] for move in self.untried_moves}\n",
    "        total = sum(move_probabilities.values())\n",
    "        probabilities = [move_probabilities[move] / total for move in self.untried_moves]\n",
    "        move = np.random.choice(self.untried_moves, p=probabilities)\n",
    "        self.untried_moves.remove(move)\n",
    "        return move\n",
    "\n",
    "    def expand(self):\n",
    "        if self.action_probabilities is None:\n",
    "            self.fetch_policy_probabilities()\n",
    "        move = self.select_probabilistic_move()\n",
    "        next_state = deepcopy(self.game_state)\n",
    "        next_state.make_move(move)\n",
    "        next_state.switch_player()\n",
    "        child_node = Node(next_state, self.policy_net, move, self)\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def update(self, result):\n",
    "        self.visits += 1\n",
    "        if result == 1:\n",
    "            self.wins += result\n",
    "\n",
    "    def best_child(self, available_moves):\n",
    "        legal_children = [child for child in self.children if child.move in available_moves]\n",
    "        choices_weights = [(child.wins / child.visits if child.visits > 0 else 0) +\n",
    "                           np.sqrt(2 * np.log(self.visits) / child.visits if child.visits > 0 else float('inf'))\n",
    "                           for child in legal_children]\n",
    "        return legal_children[choices_weights.index(max(choices_weights))] if choices_weights else None\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, iterations=1):\n",
    "        self.game = ConnectFour()\n",
    "        self.iterations = iterations\n",
    "        self.policy_net = PolicyNetwork()  # Instantiate the PolicyNetwork\n",
    "        model_path = 'path_to_trained_policy_net.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            self.policy_net.load_state_dict(torch.load(model_path))  # Load the trained model if it exists\n",
    "            print(\"Loaded trained model.\")\n",
    "        else:\n",
    "            print(\"No trained model found. Starting training from scratch.\")\n",
    "        self.policy_net.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def train(self):\n",
    "        optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=0.001)  # Setup optimizer\n",
    "        self.policy_net.train()  # Set network to training mode\n",
    "    \n",
    "        for i in range(self.iterations):\n",
    "            game = ConnectFour()\n",
    "            root = Node(game, self.policy_net)  # Initialize MCTS root\n",
    "    \n",
    "            # Collect data for training\n",
    "            game_data = []\n",
    "            while not game.is_terminal():\n",
    "                move = monte_carlo_tree_search(root, self.policy_net, max_seconds=0, max_milliseconds=300)  # MCTS to choose action\n",
    "                game.make_move(move)\n",
    "                game.switch_player()\n",
    "                # Store state, action, and provisional outcome as a list to modify later\n",
    "                game_data.append([deepcopy(game.board), move, None])  # Use list instead of tuple\n",
    "    \n",
    "            # Assign outcomes to game data after game concludes\n",
    "            outcome = 1 if game.is_winner(self.game.current_player) else 0  # Simplified outcome\n",
    "            for data in game_data:\n",
    "                data[2] = outcome  # Update the provisional None outcome with actual outcome\n",
    "    \n",
    "            # Update policy network based on collected game data\n",
    "            self.update_policy_network(game_data, optimizer)\n",
    "    \n",
    "    def update_policy_network(self, game_data, optimizer):\n",
    "        for state, action, outcome in game_data:\n",
    "            state_tensor = self.board_to_tensor(state)  # Convert board to tensor\n",
    "            print(f\"Input Tensor Shape: {state_tensor.shape}\")  # Debugging line to check shape\n",
    "            action_probabilities = self.policy_net(state_tensor) \n",
    "            loss = compute_loss(action_probabilities, action, outcome)  # Define loss function based on your needs\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def board_to_tensor(self, board):\n",
    "        # Convert board state to a 2-channel tensor\n",
    "        state_tensor = torch.zeros((1, 2, 6, 7), dtype=torch.float32)\n",
    "        for r in range(6):\n",
    "            for c in range(7):\n",
    "                player = board[r][c]\n",
    "                if player == 1:\n",
    "                    state_tensor[0, 0, r, c] = 1  # Player 1's channel\n",
    "                elif player == 2:\n",
    "                    state_tensor[0, 1, r, c] = 1  # Player 2's channel\n",
    "        return state_tensor\n",
    "\n",
    "\n",
    "\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "class PettingZooAgent:\n",
    "    def __init__(self, train_iterations=1):\n",
    "        self.trainer = Trainer(train_iterations)\n",
    "        self.trainer.train()\n",
    "\n",
    "    def update_game_state(self, observation):\n",
    "        \"\"\"Update the internal game state based on the observation from PettingZoo.\"\"\"\n",
    "        self.trainer.game.board = [[0] * 7 for _ in range(6)]  # Reset the board state\n",
    "        player1_count = 0\n",
    "        player2_count = 0\n",
    "        # Assuming the observation includes a 'board' key with a 6x7x2 tensor\n",
    "        board_state = observation['observation']\n",
    "        \n",
    "        for row in range(6):\n",
    "            for col in range(7):\n",
    "                if board_state[row, col, 0] == 1:\n",
    "                    self.trainer.game.board[row][col] = 1\n",
    "                    player1_count += 1\n",
    "                elif board_state[row, col, 1] == 1:\n",
    "                    self.trainer.game.board[row][col] = 2\n",
    "                    player2_count += 1\n",
    "\n",
    "        # Set the current player based on the count of the tokens\n",
    "        if player1_count <= player2_count:\n",
    "            self.trainer.game.current_player = 1\n",
    "        else:\n",
    "            self.trainer.game.current_player = 2\n",
    "\n",
    "    def choose_action(self, observation, action_mask):\n",
    "        self.update_game_state(observation)\n",
    "        if action_mask is None:\n",
    "            available_moves = [True for i in range(action_mask)]\n",
    "        else:\n",
    "            available_moves = [i is True for i in range(action_mask)]\n",
    "        root_node = Node(self.trainer.game, self.trainer.policy_net)\n",
    "        best_move = monte_carlo_tree_search(root_node, self.trainer.policy_net, max_seconds=1, max_milliseconds=100)\n",
    "        return best_move\n",
    "\n",
    "pz_agent = PettingZooAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43baf428-1d32-4e2b-b663-dcfd38e85e5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     cum_rewards[agent_name] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Update cumulative reward at game end\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m agent_name \u001b[38;5;241m==\u001b[39m trained_agent:\n\u001b[0;32m---> 29\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpz_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# This is for the random agent\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([i \u001b[38;5;28;01mfor\u001b[39;00m i, available \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(action_mask) \u001b[38;5;28;01mif\u001b[39;00m available])\n",
      "Cell \u001b[0;32mIn[10], line 234\u001b[0m, in \u001b[0;36mPettingZooAgent.choose_action\u001b[0;34m(self, observation, action_mask)\u001b[0m\n\u001b[1;32m    232\u001b[0m     available_moves \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(action_mask)]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     available_moves \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_mask\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    235\u001b[0m root_node \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mgame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mpolicy_net)\n\u001b[1;32m    236\u001b[0m best_move \u001b[38;5;241m=\u001b[39m monte_carlo_tree_search(root_node, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mpolicy_net, max_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_milliseconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "\n",
    "rewards = []\n",
    "# Run multiple games to evaluate performance\n",
    "for nb_run in range(10):\n",
    "    env = connect_four_v3.env()\n",
    "    env.reset()\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "    # Randomly assign the trained agent to be the first or second player\n",
    "    if random.choice([True, False]):\n",
    "        trained_agent = env.agents[0]  # Trained agent as the first player\n",
    "        random_agent = env.agents[1]  # Random agent as the second player\n",
    "    else:\n",
    "        trained_agent = env.agents[1]  # Trained agent as the second player\n",
    "        random_agent = env.agents[0]  # Random agent as the first player\n",
    "\n",
    "    cum_rewards = {trained_agent: 0, random_agent: 0}\n",
    "    \n",
    "    for agent_name in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        action_mask = observation['action_mask']\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "            cum_rewards[agent_name] += reward  # Update cumulative reward at game end\n",
    "        elif agent_name == trained_agent:\n",
    "            action = pz_agent.choose_action(observation, action_mask)\n",
    "        else:  # This is for the random agent\n",
    "            action = random.choice([i for i, available in enumerate(action_mask) if available])\n",
    "        \n",
    "        env.step(action)\n",
    "        if not (termination or truncation):  # Update cumulative rewards if the game continues\n",
    "            cum_rewards[agent_name] += reward\n",
    "\n",
    "    # Store the results from this game\n",
    "    rewards.append(cum_rewards)\n",
    "    env.close()\n",
    "\n",
    "# Print the results\n",
    "print(\"Rewards from each game:\")\n",
    "for i, result in enumerate(rewards):\n",
    "    print(f\"Game {i + 1}: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c5071-13fb-44b0-9ba4-88e36af92d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
